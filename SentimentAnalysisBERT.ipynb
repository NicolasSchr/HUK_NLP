{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "7ZTHRU-NmX1H"
      },
      "outputs": [],
      "source": [
        "# Dieser Approach finetuned BERT. Es würde wahrscheinlich schneller und mindestens gleichgut gehen wenn man nen eigenes Modell (siehe https://www.kaggle.com/code/youssefabdelghfar/twitter-sentiment-analysis-nlp-lstm/notebook),\n",
        "# oder ein dictionaryBased Modell wie Vader benutzen würde,\n",
        "# aber ich wollte schlichtweg mal ausprobieren wie gut es klappt wenn man BERT finetuned.\n",
        "# Wegen des finetunes wird eine GPU vorrausgesetzt -> alle params außer dem classifier sind gefreezed. Braucht trotzdem erstaunlich lange (ca. 7 min pro epoche auf colab)\n",
        "\n",
        "# Ich hab mir allerdings das Data cleaning und preprocessing von https://www.kaggle.com/code/youssefabdelghfar/twitter-sentiment-analysis-nlp-lstm/notebook ausgeliehen :-)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertTokenizer\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "TcwlHFwLuXwe"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "QDP5_eNsmX1I"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class\n",
        "class TweetsDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe['text']\n",
        "        self.labels = dataframe['label']\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Tokenize the input text - später maybe noch im preprocessing machen und nicht on-demand im getter\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess\n",
        "def clean_tweet(tweet):\n",
        "    # Remove URLs\n",
        "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
        "    # Remove user @ references and '#' from tweet\n",
        "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
        "    # Remove special characters, numbers, and punctuations\n",
        "    tweet = re.sub(r'\\W', ' ', tweet)\n",
        "    tweet = re.sub(r'\\d', ' ', tweet)\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "    tweet = tweet.strip()\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "q0vcsltgt6OC"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXOhZYWcmX1J",
        "outputId": "75351092-05e8-4b3a-80ae-c7ec6269e94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   labelStr                                               text  label\n",
            "0  Positive  im getting on borderlands and i will murder yo...      3\n",
            "1  Positive  i am coming to the borders and i will kill you...      3\n",
            "2  Positive  im getting on borderlands and i will kill you all      3\n",
            "3  Positive  im coming on borderlands and i will murder you...      3\n",
            "4  Positive  im getting on borderlands and i will murder yo...      3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# load data and add columns\n",
        "column_names = [\"id\",\"category\",\"labelStr\",\"text\"]\n",
        "trainingData = pd.read_csv(r\"/content/training.csv\",names=column_names)\n",
        "validationData = pd.read_csv(r\"/content/validation.csv\",names=column_names)\n",
        "\n",
        "trainingData = trainingData.drop(columns=['id', 'category'])\n",
        "validationData = validationData.drop(columns=['id', 'category'])\n",
        "\n",
        "########## Data Cleaning ausgeborgt von kaggle\n",
        "\n",
        "# clean data\n",
        "trainingData = trainingData.dropna().reset_index(drop=True) # Das wirft im Dataloader nen keyError wenn man den index nicht resettet. Das hat mich locker ne Stunde gekostet zu schauen woran das liegt. Warum wird das nicht automatisch gemacht.\n",
        "trainingData = trainingData.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# preprocess\n",
        "trainingData['text'] = trainingData['text'].apply(clean_tweet)\n",
        "validationData['text'] = validationData['text'].apply(clean_tweet)\n",
        "\n",
        "# lowercase everything\n",
        "trainingData['text'] = trainingData['text'].str.lower()\n",
        "validationData['text'] = validationData['text'].str.lower()\n",
        "\n",
        "# Drop rows where 'OriginalTweet' is empty\n",
        "trainingData = trainingData.dropna(subset=['text']).reset_index(drop=True)\n",
        "validationData = validationData.dropna(subset=['text']).reset_index(drop=True)\n",
        "\n",
        "# Alternatively, if there are rows with just whitespace, use this to remove them as well:\n",
        "trainingData = trainingData[trainingData['text'].str.strip() != ''].reset_index(drop=True)\n",
        "validationData = validationData[validationData['text'].str.strip() != ''].reset_index(drop=True)\n",
        "\n",
        "trainingData = trainingData.drop_duplicates(subset=['text']).reset_index(drop=True)\n",
        "###########\n",
        "print(len(trainingData))\n",
        "\n",
        "# convert String into int\n",
        "label_encoder = LabelEncoder()\n",
        "trainingData['label'] = label_encoder.fit_transform(trainingData[[\"labelStr\"]])\n",
        "validationData['label'] = label_encoder.transform(validationData[[\"labelStr\"]])\n",
        "\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "MAX_LEN = 128  # Length should be enough as tweets are limited in length anyway\n",
        "\n",
        "# create datasets\n",
        "train_dataset = TweetsDataset(trainingData, tokenizer, MAX_LEN)\n",
        "val_dataset = TweetsDataset(validationData, tokenizer, MAX_LEN)\n",
        "print(trainingData.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "foFmDVXtmX1K"
      },
      "outputs": [],
      "source": [
        "# create Dataloader\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZyzANCdmX1K",
        "outputId": "ede4e473-27f0-4eed-b7dc-7ab91f2bae96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# load model and set output to the 4 classes\n",
        "# This will cause the parameter \"warnings\" below, as its not originally trained on 4 classes.\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "\n",
        "\n",
        "# freeze all params except for the classifier one (classifier heißt: model.classifier.*)\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8efnOuBmX1L",
        "outputId": "1b859917-2782-4887-ffe5-6df838b93738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "# optimizer and loss\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01  ,correct_bias=False) # AdamW optimizer because is rarely not good lol\n",
        "loss_fn = CrossEntropyLoss() # Cross entropy is standard bert loss\n",
        "\n",
        "# move to gpu\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(torch.cuda.is_available())\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "iesns7i9mX1M"
      },
      "outputs": [],
      "source": [
        "# train method\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in tqdm(data_loader):\n",
        "\n",
        "        # Move input tensors to device (GPU/CPU)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "        # Collect predictions and labels for F1 score calculation\n",
        "        all_predictions.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate the average loss and accuracy\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = total_correct / len(data_loader.dataset)\n",
        "\n",
        "    # Calculate the F1 score\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, f1\n",
        "\n",
        "# eval method\n",
        "def eval_model(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "            # Collect predictions and labels for F1 score calculation\n",
        "            all_predictions.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate the average loss and accuracy\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = total_correct / len(data_loader.dataset)\n",
        "\n",
        "    # Calculate the F1 score\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XAcbenWmX1M",
        "outputId": "4c5e690c-032b-4bf1-8244-ceaedb27d710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2075/2075 [07:44<00:00,  4.47it/s]\n",
            "100%|██████████| 32/32 [00:07<00:00,  4.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.261530587587012, accuracy: 0.4441615954690607, F1: 0.4172665033798929\n",
            "Validation loss: 1.156990835443139, accuracy: 0.528, F1: 0.4938582249817842\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2075/2075 [07:58<00:00,  4.33it/s]\n",
            "100%|██████████| 32/32 [00:07<00:00,  4.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.224975165257971, accuracy: 0.4680966439718021, F1: 0.44208599357681255\n",
            "Validation loss: 1.1689600814133883, accuracy: 0.477, F1: 0.4499318404128305\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2075/2075 [07:55<00:00,  4.36it/s]\n",
            "100%|██████████| 32/32 [00:07<00:00,  4.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.2170501064392458, accuracy: 0.47297704404410434, F1: 0.4489657134316694\n",
            "Validation loss: 1.1978987660259008, accuracy: 0.506, F1: 0.4625993374245612\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2075/2075 [07:55<00:00,  4.36it/s]\n",
            "100%|██████████| 32/32 [00:06<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.2151943658058901, accuracy: 0.47231427366391515, F1: 0.4491744151914261\n",
            "Validation loss: 1.136104928329587, accuracy: 0.527, F1: 0.5028788179338414\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 1997/2075 [07:37<00:18,  4.32it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train / Eval / Save dict\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "    train_loss, train_acc, train_f1 = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
        "    val_loss, val_acc, val_f1 = eval_model(model, val_loader, loss_fn, device)\n",
        "\n",
        "    print(f\"Train loss: {train_loss}, accuracy: {train_acc}, F1: {train_f1}\")\n",
        "    print(f\"Validation loss: {val_loss}, accuracy: {val_acc}, F1: {val_f1}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# BERT scheint tatsächlich nicht so gut hierfür zu klappen.\n",
        "# Ich hab (tatsächlich erst nachdem ich das geschrieben hab). Mal bei Kaggle nach anderen Bert Ansätzen für das Dataset geschaut,\n",
        "# und da kommen ähnliche Ergebnisse raus wie hier. https://www.kaggle.com/code/kirollosashraf/twitter-sentiment-analysis-nlp-using-bert\n",
        "# Ich habe leider auch nicht groß hyperparameter tuning betrieben, weil mein Cuda lokal irgendwie kaputt gegangen ist, und ich ich bei colab im Nutzungslimit bin.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}